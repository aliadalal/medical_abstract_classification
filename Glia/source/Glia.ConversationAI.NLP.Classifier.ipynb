{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46ecb8d",
   "metadata": {},
   "source": [
    "# Glia.ConversationAI.NLP.TextClassifier\n",
    "## Aliasgher Dalal\n",
    "## 2023-11-20\n",
    "\n",
    "The project is done as a take home assignment from Glia Inc. given as part of the assessment for the role of Data Scientist: Conversational AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22675c48",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "cVectorize = CountVectorizer()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding,LSTM,Embedding,SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#from keras.layers import Embedding, LSTM, Dense,SpatialDropout1D\n",
    "#from keras.models import Sequential\n",
    "from keras import utils as np_utils\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tkinter import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98b216",
   "metadata": {},
   "source": [
    "## Business Understanding & Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf4f10",
   "metadata": {},
   "source": [
    "The business goal is to develop an application that can categorize a medical abstract into one of the five conditions. Each class represent current medical condition of a patient. These conditions are:  \n",
    "These classes \n",
    "    1\tneoplasms\n",
    "\t2\tdigestive system diseases\n",
    "\t3\tnervous system diseases\n",
    "\t4\tcardiovascular diseases\n",
    "\t5\tgeneral pathological conditions\n",
    "To aid in the development of the application, dataset containing ~28k labelled abstracts is provided. \n",
    "\n",
    "Initial analysis shows that a text classification machine learning technique may be applied to develop an application, which when given an abstract will be capable of returning the most likely condition that the medical abstract is referring to in the text and hence the patient associated to the abstract is currently afflicted to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa074fe",
   "metadata": {},
   "source": [
    "## Data Understanding & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47905bd2",
   "metadata": {},
   "source": [
    "This section of the notebook processes data, analyses and prepares data for the next stage of classification i.e. machine learning modelling.\n",
    "\n",
    "Data is ingested - in this case Kaggle dataset for medical abstracts is ingested. The data is then processed through various claeaning stages including removal of missing data. In addition, the abstract text is converted to lower case, and split into word tokens. The stopwords are then removed from the text and lemmatization is performed before putting the processed word tokens back into string. The string represent processed medical abstract ready for further processing.\n",
    "\n",
    "Finally, the processed text is vectorized for input into the machine learnign model(s).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20bf0f",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "#### Source: https://www.kaggle.com/datasets/chaitanyakck/medical-text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77651022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ingest data from csv into pandas dataframe\n",
    "data_folder = \"/Users/aliasgherdalal/Documents/Glia/dataset\"\n",
    "fLabel = data_folder + \"/medical_tc_labels.csv\"\n",
    "fTrain = data_folder + \"/medical_tc_train.csv\"\n",
    "fTest  = data_folder + \"/medical_tc_test.csv\"\n",
    "\n",
    "dfLabel=pd.read_csv(fLabel)\n",
    "dfTrain=pd.read_csv(fTrain)\n",
    "dfTest=pd.read_csv(fTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfLabel.head(),'\\n',dfLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTrain.head(),'\\n',dfTrain.shape)\n",
    "dfTrain['condition_label'].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfTest.head(),'\\n',dfTest.shape)\n",
    "dfTest['condition_label'].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836d40a",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9bb72",
   "metadata": {},
   "source": [
    "#### Basic Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Values\n",
    "\n",
    "print('Training Set: \\n',dfTrain.isna().sum(),'Test Set: \\n',dfTest.isna().sum(),'Labels: \\n',dfLabel.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb1dbe",
   "metadata": {},
   "source": [
    "#### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(df):\n",
    "    # simply converts a pandas column into a list for processing.\n",
    "    return df.tolist()\n",
    "def dataProcessor(dfList):\n",
    "    # Performs text processing on a list of text string (medical abstracts) and returns a processed list \n",
    "    # for vectorizing for machine learning.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    containerDoc=[]\n",
    "    for i in range(len(dfList)):\n",
    "        tempDoc=dfList[i]\n",
    "        tempDoc=tempDoc.lower() # lower case\n",
    "        tempDoc = tempDoc.split() # split into words\n",
    "        tempDoc = [word for word in tempDoc if word not in stopwords.words('english')] # remove stops words\n",
    "        tempDoc = [lemmatizer.lemmatize(word) for word in tempDoc] #Lemmatize\n",
    "        tempDoc = ' '.join(tempDoc) # recreate the doc\n",
    "        containerDoc.append(tempDoc)\n",
    "    return containerDoc\n",
    "def dataProcessorString(dfString):\n",
    "    # Performs text processing on a text string (a medical abstract) and returns processed string \n",
    "    # for vectorizing for machine learning inference.    \n",
    "    tempDoc=dfString\n",
    "    tempDoc=tempDoc.lower() # lower case\n",
    "    tempDoc = tempDoc.split() # split into words\n",
    "    tempDoc = [word for word in tempDoc if word not in stopwords.words('english')] # remove stops words\n",
    "    tempDoc = [lemmatizer.lemmatize(word) for word in tempDoc] #Lemmatize\n",
    "    tempDoc = ' '.join(tempDoc) # recreate the doc\n",
    "    return tempDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfListTrain=dataProcessor(dfTrain['medical_abstract'])\n",
    "dfListTest=dataProcessor(dfTest['medical_abstract'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e13860",
   "metadata": {},
   "source": [
    "#### Vectorize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08951515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizerFit(dfX,dfY):\n",
    "    dfX=cVectorize.fit_transform(dfX)\n",
    "    dfY=dfY.tolist()\n",
    "    dfY=np.array(dfY)\n",
    "    return dfX,dfY\n",
    "x_train_counts,condition_label_list_train=vectorizerFit(dfListTrain,dfTrain['condition_label'])\n",
    "def vectorizerTransform(dfX,dfY):\n",
    "    dfX = cVectorize.transform(dfX)\n",
    "    dfY=dfY.tolist()\n",
    "    dfY=np.array(dfY)\n",
    "    return dfX,dfY\n",
    "x_test_counts,condition_label_list_test=vectorizerTransform(dfListTest,dfTest['condition_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab38de",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028652c",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994fa0e",
   "metadata": {},
   "source": [
    "### Model 1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(x_train_counts, condition_label_list_train)\n",
    "\n",
    "y_score = clf.predict(x_test_counts)\n",
    "\n",
    "n_right = 0\n",
    "for i in range(len(y_score)):\n",
    "    if y_score[i] == condition_label_list_test[i]:\n",
    "        n_right += 1\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % ((n_right/float(len(condition_label_list_test)) * 100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb1e00",
   "metadata": {},
   "source": [
    "### Model 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa760b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(solver = \"saga\")\n",
    "LR.fit(x_train_counts,condition_label_list_train)\n",
    "pred_lr=LR.predict(x_test_counts) # Here is where I get an error\n",
    "count=0\n",
    "for i in range (2888):\n",
    "    pred_lrOne=LR.predict(x_test_counts[i])\n",
    "    if (dfTest['condition_label'].values[i] == pred_lrOne ):\n",
    "        count=count+1\n",
    "print(np.round(count/28.88),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7dd1ec",
   "metadata": {},
   "source": [
    "## Model 3 - Deep Learning LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a585e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dXTrain= dfListTrain\n",
    "dXTest = dfListTest\n",
    "dYTrain = dfTrain['condition_label'].to_list()\n",
    "dYTest = dfTest['condition_label'].to_list()\n",
    "\n",
    "num_classes = len(set(dYTrain)) # number of classes\n",
    "max_words = 10000 # max number of words to use in the vocabulary MAX_NB_WORDS\n",
    "max_len = 250 # max length of each text (in terms of number of words)MAX_SEQUENCE_LENGTH\n",
    "embedding_dim = 100 # dimension of word embeddings\n",
    "lstm_units = 64 # number of units in the LSTM layer\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Tokenize the training set\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dXTrain)\n",
    "sequences = tokenizer.texts_to_sequences(dXTrain)\n",
    "dXTrain = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Tokenize the test set\n",
    "tokenizer.fit_on_texts(dXTest)\n",
    "sequences= tokenizer.texts_to_sequences(dXTest)\n",
    "dXTest = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "\n",
    "dYTrain = [x - 1 for x in dYTrain]\n",
    "dYTrain=np.array(dYTrain)\n",
    "\n",
    "dYTest = [x - 1 for x in dYTest]\n",
    "dYTest=np.array(dYTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929790c8",
   "metadata": {},
   "source": [
    "#### Model 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDL1 = Sequential()\n",
    "modelDL1.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "modelDL1.add(SpatialDropout1D(0.2))\n",
    "modelDL1.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "modelDL1.add(Dense(num_classes, activation='softmax'))\n",
    "modelDL1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDL1 = modelDL1.fit(dXTrain, dYTrain, epochs=epochs, batch_size=batch_size,validation_data=(dXTest,dYTest),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efe35f",
   "metadata": {},
   "source": [
    "#### Model 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDL2 = Sequential()\n",
    "modelDL2.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "modelDL2.add(LSTM(lstm_units))\n",
    "modelDL2.add(Dense(num_classes, activation='softmax'))\n",
    "modelDL2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDL2 = modelDL2.fit(dXTrain, dYTrain, epochs=epochs, batch_size=batch_size,validation_data=(dXTest,dYTest),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbadfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef003c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsT = dfTest['condition_label'].to_list()\n",
    "labelsT = [x - 1 for x in labelsT]\n",
    "labelsT=np.array(labelsT)\n",
    "sequences = tokenizer.texts_to_sequences(dfListTest)\n",
    "xT = pad_sequences(sequences, maxlen=max_len)\n",
    "model.evaluate(x=xT, y=labelsT)\n",
    "#result=model.predict(xT)\n",
    "#result,labelsT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f638a2f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8ba51",
   "metadata": {},
   "source": [
    "### Model 1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70470dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions.\n",
    "cnf_matrix = metrics.confusion_matrix(condition_label_list_test, y_score)\n",
    "print(\"Confusion matrix\\n\",cnf_matrix)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(condition_label_list_test, y_score))\n",
    "print(\"Precision:\",metrics.precision_score(condition_label_list_test, y_score,average='weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(condition_label_list_test, y_score,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap((cnf_matrix), annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e3c76",
   "metadata": {},
   "source": [
    "### Model 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cfdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions.\n",
    "cnf_matrix = metrics.confusion_matrix(condition_label_list_test, pred_lr)\n",
    "print(\"Confusion matrix\\n\",cnf_matrix)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(condition_label_list_test, pred_lr))\n",
    "print(\"Precision:\",metrics.precision_score(condition_label_list_test, pred_lr,average='weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(condition_label_list_test, pred_lr,average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19952976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap((cnf_matrix), annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251e80d",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058138f7",
   "metadata": {},
   "source": [
    "### Model 1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ea6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_fileNB = \"NBTextClass.pkl\"  \n",
    "vec_file = 'vectorizer.pickle'\n",
    "pickle.dump(cVectorize, open(vec_file, 'wb'))\n",
    "with open(model_pkl_fileNB, 'wb') as file:  \n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60606e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pickle file\n",
    "with open(model_pkl_fileNB, 'rb') as file:  \n",
    "    modelUploaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc0e5e",
   "metadata": {},
   "source": [
    "### Model 2 - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b86c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the iris classification model as a pickle file\n",
    "model_pkl_fileLR = \"LRTextClass.pkl\"  \n",
    "#vec_file = 'vectorizer.pickle'\n",
    "#pickle.dump(cVectorize, open(vec_file, 'wb'))\n",
    "with open(model_pkl_fileLR, 'wb') as file:  \n",
    "    pickle.dump(LR, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3276ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pickle file\n",
    "with open(model_pkl_fileLR, 'rb') as file:  \n",
    "    modelUploaded = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc1c0f",
   "metadata": {},
   "source": [
    "### Model 3 - DL LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07811dfe",
   "metadata": {},
   "source": [
    "#### Model 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bba682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the iris classification model as a pickle file\n",
    "model_pkl_fileDL1 = \"DL1TextClass.pkl\"  \n",
    "#vec_file = 'vectorizer.pickle'\n",
    "#pickle.dump(cVectorize, open(vec_file, 'wb'))\n",
    "with open(model_pkl_fileDL1, 'wb') as file:  \n",
    "    pickle.dump(modelDL1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f42d0d",
   "metadata": {},
   "source": [
    "#### Model 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the iris classification model as a pickle file\n",
    "model_pkl_fileDL2 = \"DL2TextClass.pkl\"  \n",
    "#vec_file = 'vectorizer.pickle'\n",
    "#pickle.dump(cVectorize, open(vec_file, 'wb'))\n",
    "with open(model_pkl_fileDL2, 'wb') as file:  \n",
    "    pickle.dump(modelDL2, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1caa69",
   "metadata": {},
   "source": [
    "# ****************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
